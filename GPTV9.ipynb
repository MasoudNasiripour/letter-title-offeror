{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mnasiri/masoud_nasiripour/second_phase/gpt/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset as BaseDataset\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from transformers import GPT2Tokenizer, AutoModelForCausalLM, GPT2LMHeadModel, AutoTokenizer\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "device=\"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bolbolzaban/gpt2-persian\")\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<letter>\", \"</letter>\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"emb_dim\" : 768,\n",
    "    \"letter_emb_dim\": 1024,\n",
    "    \"vocab_size\" : tokenizer.vocab_size,\n",
    "    \"save_path\": \"./models/v9.pth\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(BaseDataset):\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = load_dataset(\"csv\", data_files=\"datasets/datasets_new.csv\")[\"train\"]\n",
    "\n",
    "    def __getitem__(self, ix):\n",
    "        item = self.data[ix]\n",
    "        return item\n",
    "\n",
    "\n",
    "    def __len__(self, ):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCollator:\n",
    "    def __call__(self, batch):\n",
    "        titles = [\"</letter>\" + item[\"title\"] for item in batch if item is not None]\n",
    "        ctx_embs = torch.tensor([json.loads(item[\"context_embedding\"]) for item in batch if item is not None], dtype=torch.float)\n",
    "\n",
    "        tokenized_title = tokenizer(titles,\n",
    "                  padding=\"max_length\",\n",
    "                  truncation=True,\n",
    "                  return_tensors=\"pt\",\n",
    "                  max_length=256)\n",
    "        \n",
    "        attention_mask = torch.stack([torch.cat([torch.tensor([1,], dtype=torch.long), mask], dim=-1) for mask in tokenized_title[\"attention_mask\"]]).to(device)\n",
    "        input_ids = tokenized_title[\"input_ids\"][:, 1:-1].long()\n",
    "        targets = tokenized_title[\"input_ids\"][:, 1:]\n",
    "        targets = torch.cat([torch.ones(targets.shape[0], 1).fill_(-100).long(), targets.masked_fill(targets == tokenizer.pad_token_id, -100)], dim=1)\n",
    "        targets[:, 2] = -100\n",
    "\n",
    "        return {\n",
    "            \"attention_mask\": attention_mask[:, :-1].to(device),\n",
    "            \"letter_emb\": ctx_embs.to(device),\n",
    "            \"input_ids\": input_ids.to(device),\n",
    "            \"label\": targets.to(device)\n",
    "        }\n",
    "\n",
    "train_dataset, eval_dataset = random_split(Dataset(tokenizer), [0.97, 0.03])\n",
    "collator_fn = CustomCollator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check model existance...\n",
      "Loading the model...\n",
      "number of trainable params:40,109,824\n",
      "loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, tokenizer, config):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.letter_projection = nn.Sequential(nn.Linear(config[\"letter_emb_dim\"], config[\"letter_emb_dim\"] // 2),\n",
    "                                                nn.Linear(config[\"letter_emb_dim\"] // 2, config[\"emb_dim\"]))\n",
    "        self.gpt = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "        self.gpt.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "        tp = 0\n",
    "        for p in self.letter_projection.parameters():\n",
    "            tp += p.numel()\n",
    "            p.requires_grad=True\n",
    "        for p in self.gpt.lm_head.parameters():\n",
    "            tp += p.numel()\n",
    "            p.requires_grad=True\n",
    "        for p in self.gpt.transformer.wte.parameters():\n",
    "            tp += p.numel()\n",
    "            p.requires_grad=True\n",
    "        for p in self.gpt.transformer.wpe.parameters():\n",
    "            tp += p.numel()\n",
    "            p.requires_grad=True\n",
    "        \n",
    "        print(f\"number of trainable params:{tp:,}\")\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, tokenizer, config):\n",
    "        print(\"check model existance...\")\n",
    "        if os.path.isfile(config[\"save_path\"]):\n",
    "            print(\"Loading the model...\")\n",
    "            self = cls(tokenizer, config)\n",
    "            self.load_state_dict(torch.load(config[\"save_path\"], weights_only=True))\n",
    "            print(\"loaded successfully!\")\n",
    "        else:\n",
    "            print(f\"couldn't find the {config['save_path']} file!\")\n",
    "            print(\"Creating a new model...\")\n",
    "            self = cls(tokenizer, config)\n",
    "        return self\n",
    "\n",
    "    def save(self, ):\n",
    "        torch.save(self.state_dict(), config[\"save_path\"])\n",
    "        print(f\"Model saved at {config['save_path']}!\")\n",
    "    \n",
    "    def forward(self, attention_mask, letter_emb, input_ids, label):\n",
    "        start_letter_token_id = tokenizer(\"<letter>\", return_tensors='pt')[\"input_ids\"][:, 1:2].to(device)\n",
    "        start_letter_token_emb = self.gpt.transformer.wte(start_letter_token_id).to(device)\n",
    "        letter_emb = self.letter_projection(letter_emb).unsqueeze(1).to(device)\n",
    "        x = self.gpt.transformer.wte(input_ids)\n",
    "        x = torch.cat([start_letter_token_emb.repeat(x.shape[0], 1, 1), letter_emb, x], dim=1)\n",
    "        x += self.gpt.transformer.wpe(torch.arange(x.shape[1]).to(device))\n",
    "        output = self.gpt(inputs_embeds=x,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True,\n",
    "            labels=label\n",
    "        )\n",
    "        return output\n",
    "\n",
    "    \n",
    "    @torch.no_grad\n",
    "    def generate(self, letter_emb):\n",
    "        model.eval()\n",
    "        letter_emb = torch.tensor(json.loads(letter_emb)).view(1,1,-1).to(device)\n",
    "        letter_emb = self.letter_projection(letter_emb)\n",
    "        output = model.gpt.generate(\n",
    "        inputs_embeds=letter_emb,\n",
    "        attention_mask=torch.ones((1, 1), dtype=torch.long).to(device),\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        temperature=0.9,\n",
    "        num_beams=5,\n",
    "        max_length=128,\n",
    "        min_length=1,\n",
    "        repetition_penalty=1.0,\n",
    "        length_penalty=1.0,\n",
    "        num_return_sequences=1,)\n",
    "        return self.tokenizer.batch_decode(output)\n",
    "\n",
    "model = Model.from_pretrained(tokenizer, config)\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = TrainingArguments(\n",
    "    output_dir=\"./cache/\",\n",
    "    learning_rate=1e-3,\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    logging_strategy=\"steps\",\n",
    "    eval_strategy=\"steps\",\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_pin_memory=False,\n",
    "    save_safetensors=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "        args=train_args,\n",
    "        data_collator=collator_fn,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='808' max='808' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [808/808 17:31, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>7.018200</td>\n",
       "      <td>5.364950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>5.188700</td>\n",
       "      <td>5.016734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>4.878200</td>\n",
       "      <td>4.721632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>4.707700</td>\n",
       "      <td>4.440592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.427700</td>\n",
       "      <td>4.238565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>4.260800</td>\n",
       "      <td>4.098491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>4.177900</td>\n",
       "      <td>3.912942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.939100</td>\n",
       "      <td>3.829328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.754800</td>\n",
       "      <td>3.712672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.660800</td>\n",
       "      <td>3.604990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>3.606400</td>\n",
       "      <td>3.547071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>3.544100</td>\n",
       "      <td>3.485258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>3.499600</td>\n",
       "      <td>3.395226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>3.405000</td>\n",
       "      <td>3.373304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.388000</td>\n",
       "      <td>3.292962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>3.273000</td>\n",
       "      <td>3.258601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>3.202800</td>\n",
       "      <td>3.217091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>3.242900</td>\n",
       "      <td>3.168721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>3.172400</td>\n",
       "      <td>3.156101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.199600</td>\n",
       "      <td>3.094691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>3.129800</td>\n",
       "      <td>3.044047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>3.097400</td>\n",
       "      <td>3.009637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>3.018900</td>\n",
       "      <td>2.994978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>3.123400</td>\n",
       "      <td>2.973544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>3.058500</td>\n",
       "      <td>2.959919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>3.018800</td>\n",
       "      <td>2.915148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>2.912000</td>\n",
       "      <td>2.910449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>3.026600</td>\n",
       "      <td>2.879662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>2.916100</td>\n",
       "      <td>2.871789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.929100</td>\n",
       "      <td>2.851527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>2.896700</td>\n",
       "      <td>2.830320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>2.804000</td>\n",
       "      <td>2.826697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>2.850900</td>\n",
       "      <td>2.812282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>2.890800</td>\n",
       "      <td>2.789882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.880400</td>\n",
       "      <td>2.764232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>2.824800</td>\n",
       "      <td>2.754507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>2.913900</td>\n",
       "      <td>2.733805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>2.776100</td>\n",
       "      <td>2.720729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>2.751100</td>\n",
       "      <td>2.706981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.749500</td>\n",
       "      <td>2.694940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>2.501000</td>\n",
       "      <td>2.709595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>2.619900</td>\n",
       "      <td>2.690870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>2.618300</td>\n",
       "      <td>2.668552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>2.604100</td>\n",
       "      <td>2.681525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>2.556800</td>\n",
       "      <td>2.659736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>2.466600</td>\n",
       "      <td>2.655886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>2.562100</td>\n",
       "      <td>2.639431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>2.581400</td>\n",
       "      <td>2.636516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>2.569700</td>\n",
       "      <td>2.604183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.484300</td>\n",
       "      <td>2.614642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>2.464100</td>\n",
       "      <td>2.605243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>2.465700</td>\n",
       "      <td>2.599665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>2.513500</td>\n",
       "      <td>2.580245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>2.485000</td>\n",
       "      <td>2.578992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>2.383300</td>\n",
       "      <td>2.572926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>2.520500</td>\n",
       "      <td>2.546465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>2.382000</td>\n",
       "      <td>2.563297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>2.465800</td>\n",
       "      <td>2.531691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>2.455200</td>\n",
       "      <td>2.529453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.388200</td>\n",
       "      <td>2.520772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>2.412000</td>\n",
       "      <td>2.522456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>2.409900</td>\n",
       "      <td>2.522459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>2.419700</td>\n",
       "      <td>2.502901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>2.566900</td>\n",
       "      <td>2.502266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>2.367300</td>\n",
       "      <td>2.485818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>2.396300</td>\n",
       "      <td>2.487540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>2.327500</td>\n",
       "      <td>2.481598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>2.329200</td>\n",
       "      <td>2.475323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>2.394200</td>\n",
       "      <td>2.472033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.396500</td>\n",
       "      <td>2.466518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>2.439600</td>\n",
       "      <td>2.451231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>2.342000</td>\n",
       "      <td>2.445079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>2.408500</td>\n",
       "      <td>2.446568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>2.442200</td>\n",
       "      <td>2.440845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>2.433800</td>\n",
       "      <td>2.437119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>2.387900</td>\n",
       "      <td>2.437600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>2.306300</td>\n",
       "      <td>2.430136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>2.386400</td>\n",
       "      <td>2.430869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>2.390200</td>\n",
       "      <td>2.428374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.352100</td>\n",
       "      <td>2.426262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at ./models/v9.pth!\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "model.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated title: : بهروز نرمافزار گامالکونیک شرکت‌تر\n",
      "True title: فرصت سرمایه‌گذاری: پیشگامی در بکارگیری تکنولوژی‌های نوین برای تحول بنیادین کسب‌وکارها\n",
      "context: بسمه تعالی\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "شماره: \n",
      "\n",
      "تاریخ: \n",
      "۰۵ / ۰۸ / ۹۵\n",
      "پیوست: \n",
      "دارد\n",
      "\n",
      "\n",
      "\n",
      "مدیریت محترم شرکت سرمایه گذاری گروه توسعه ملی\n",
      "\n",
      "باسلام و احترام\n",
      "پیرو نامه شماره ۹۴۷۳۲ و مذاکرات انجام‌شده، احتراما فرصت را مغتنم شمرده باستحضار می‏رساند شرکت‎گام‎الکترونیک طی ۲۸ سال سابقه‎ اجرایی، همواره در بکارگیری آخرین تکنولوژی‌های روز پیشگام بوده و در این راستا بزرگترین پروژه‎های نرم‌افزاری کشور را در پیشینه پرافتخار خود ثبت نموده است. هم‌اکنون نرم‏افزار اتوماسیون اداری الماس‌گام در اکثریت بانک‌ها و سازمان‌های بزرگ و کوچک کشور علاوه بر کاهش فزاینده زمان رسیدگی به امور، باعث تقلیل یا حذف قابل‌ملاحظه فرآیندهای کاغذی شده است. این شرکت با برخورداری از تیم‎های متخصص و پرتوان طی سال‎های متمادی توانسته با ایجاد بستری مناسب در زیرساخت فرآیندهای کاری سازمان‌ها، تحول بنیادین ایجاد نماید و با اجرای پروژه‏های اتوماسیون اداری متمرکز کشوری سازمان‎های پیشرو را در ارائه خدمات ارزنده به مشتریان خود یاری دهد. \n",
      "بیش از یک دهه است که در این راستا شرکت گام‏الکترونیک سیستم اتوماسیون اداری الماس‌گام را با تکنولوژی روز و کاملا مبتنی بر وب طراحی و در سازمان‏های بزرگ و کوچک عملیاتی کرده تا جایگزین نسل قبلی محصول این شرکت (نرم‏افزار گردش الکترونیکی مکاتبات) شود. طی این مدت اکثریت مشتریان نسخه‌های کلاینتی اقدام به مهاجرت به الماس گام نموده و از مزایای آن بهره برده‌اند. \n",
      "با توجه به محدودیت‌های تکنولوژی قبلی توسعه قابلیت‌های پیشرفته و انتظارات جدید کاربران در آن بستر محیا نبوده و تنها مانع رشد کسب و کار سازمان مشتری خواهد بود.. \n",
      "بدینوسیله ضمن اعلام آمادگی جهت نصب و راه‌اندازی سیستم اتوماسیون اداری مبتنی بر وب الماس‌گام در آن مجموعه محترم به پیوست پیشنهاد قیمت تعدیل‌شده این شرکت جهت استحضار تقدیم می‌گردد. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ix = 4\n",
    "sample_data = eval_dataset[ix]\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    elt = model.gpt.transformer.wte(tokenizer(\"</letter>\", return_tensors='pt')[\"input_ids\"][:, 1:2].to(device))\n",
    "    slt = model.gpt.transformer.wte(tokenizer(\"<letter>\", return_tensors='pt')[\"input_ids\"][:, 1:2].to(device))\n",
    "    letter_emb = model.letter_projection(torch.tensor(json.loads(sample_data[\"context_embedding\"])).to(device).view(1, 1,-1))\n",
    "    letter_emb = torch.cat([slt, letter_emb, elt], dim=1)\n",
    "    letter_emb += model.gpt.transformer.wpe(torch.arange(letter_emb.shape[1]).to(device))\n",
    "    output = model.gpt.generate(\n",
    "        inputs_embeds=letter_emb,\n",
    "        attention_mask=torch.ones((1, 1), dtype=torch.long).to(device),\n",
    "        do_sample=True,\n",
    "        top_p=0.5,\n",
    "        temperature=0.5,\n",
    "        num_beams=5,\n",
    "        max_length=16,\n",
    "        min_length=1,\n",
    "        repetition_penalty=1.0,\n",
    "        length_penalty=1.0,\n",
    "        num_return_sequences=1,\n",
    "    )\n",
    "\n",
    "    output_ids = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "    print('Generated title:', ''.join(output_ids))\n",
    "    print('True title:', sample_data[\"title\"])\n",
    "    print('context:', sample_data[\"context\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
