{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mnasiri/masoud_nasiripour/second_phase/gpt/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset as BaseDataset\n",
    "from torch.utils.data import DataLoader as BaseDataLoader\n",
    "\n",
    "from transformers import GPT2Tokenizer, AutoModelForCausalLM, GPT2LMHeadModel\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "device=\"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"emb_dim\" : 768,\n",
    "    \"letter_emb_dim\": 1024,\n",
    "    \"vocab_size\" : tokenizer.vocab_size,\n",
    "    \"save_path\": \"./models/v1.pth\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(BaseDataset):\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = load_dataset(\"csv\", data_files=\"dataset.csv\")[\"train\"]\n",
    "\n",
    "    def __getitem__(self, ix):\n",
    "        item = self.data[ix]\n",
    "        return item\n",
    "\n",
    "\n",
    "    def __len__(self, ):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCollator:\n",
    "    def __call__(self, batch):\n",
    "        titles = [item[\"title\"] for item in batch if item is not None]\n",
    "        ctx_embs = torch.tensor([json.loads(item[\"context_embedding\"]) for item in batch if item is not None], dtype=torch.float)\n",
    "\n",
    "        tokenized_title = tokenizer(titles,\n",
    "                  padding=\"longest\",\n",
    "                  truncation=True,\n",
    "                  return_tensors=\"pt\")\n",
    "        \n",
    "        attention_mask = torch.stack([torch.cat([torch.tensor([1,], dtype=torch.long), mask], dim=-1) for mask in tokenized_title[\"attention_mask\"]]).to(device)\n",
    "\n",
    "        input_ids = tokenized_title[\"input_ids\"][:, :-1].long()\n",
    "        targets = tokenized_title[\"input_ids\"]\n",
    "        targets = targets.masked_fill(targets == tokenizer.pad_token_id, -100)\n",
    "\n",
    "        return {\n",
    "            \"attention_mask\": attention_mask[:, :-1].to(device),\n",
    "            \"letter_emb\": ctx_embs.to(device),\n",
    "            \"input_ids\": input_ids.to(device),\n",
    "            \"label\": targets.to(device)\n",
    "        }\n",
    "\n",
    "dataset = Dataset(tokenizer)\n",
    "collator_fn = CustomCollator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check model existance...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (letter_projection): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "    (1): Linear(in_features=2048, out_features=768, bias=True)\n",
       "  )\n",
       "  (gpt): GPT2LMHeadModel(\n",
       "    (transformer): GPT2Model(\n",
       "      (wte): Embedding(50257, 768)\n",
       "      (wpe): Embedding(1024, 768)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0-11): 12 x GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D(nf=2304, nx=768)\n",
       "            (c_proj): Conv1D(nf=768, nx=768)\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D(nf=3072, nx=768)\n",
       "            (c_proj): Conv1D(nf=768, nx=3072)\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, tokenizer, config):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.letter_projection = nn.Sequential(nn.Linear(config[\"letter_emb_dim\"], config[\"letter_emb_dim\"] * 2),\n",
    "                                                nn.Linear(config[\"letter_emb_dim\"] * 2, config[\"emb_dim\"]))\n",
    "        self.gpt = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "        for p in self.parameters():\n",
    "            p.requires_grad=True\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, tokenizer, config):\n",
    "        print(\"check model existance...\")\n",
    "        if os.path.isfile(config[\"save_path\"]):\n",
    "            print(\"Loading the model...\")\n",
    "            self = cls(tokenizer, config)\n",
    "            self.load_state_dict(torch.load(config[\"save_path\"], weights_only=True))\n",
    "            print(\"loaded successfully!\")\n",
    "        else:\n",
    "            print(f\"couldn't find the {config['save_path']} file!\")\n",
    "            print(\"Creating a new model...\")\n",
    "            self = cls(tokenizer, config)\n",
    "        return self\n",
    "\n",
    "    def save(self, ):\n",
    "        torch.save(self.state_dict(), config[\"save_path\"])\n",
    "        print(f\"Model saved at {config['save_path']}!\")\n",
    "    \n",
    "    def forward(self, attention_mask, letter_emb, input_ids, label):\n",
    "        letter_emb = self.letter_projection(letter_emb).unsqueeze(1)\n",
    "        x = self.gpt.transformer.wte(input_ids)\n",
    "        x += self.gpt.transformer.wpe(torch.arange(x.shape[1]).to(device))\n",
    "        x = torch.cat([letter_emb, x], dim=1)\n",
    "\n",
    "        output = self.gpt(inputs_embeds=x,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True,\n",
    "            labels=label\n",
    "        )\n",
    "        return output\n",
    "\n",
    "    \n",
    "    @torch.no_grad\n",
    "    def generate(self, letter_emb):\n",
    "        model.eval()\n",
    "        letter_emb = torch.tensor(json.loads(letter_emb)).view(1,1,-1).to(device)\n",
    "        letter_emb = self.letter_projection(letter_emb)\n",
    "        output = model.gpt.generate(\n",
    "        inputs_embeds=letter_emb,\n",
    "        attention_mask=torch.ones((1, 1), dtype=torch.long).to(device),\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        temperature=0.9,\n",
    "        num_beams=5,\n",
    "        max_length=128,\n",
    "        min_length=1,\n",
    "        repetition_penalty=1.0,\n",
    "        length_penalty=1.0,\n",
    "        num_return_sequences=1,)\n",
    "\n",
    "model = Model.from_pretrained(tokenizer, config)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = TrainingArguments(\n",
    "    output_dir=\"./cache/\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=16,\n",
    "    num_train_epochs=80,\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=20,\n",
    "    logging_strategy=\"steps\",\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_pin_memory=False,\n",
    "    save_safetensors=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "        args=train_args,\n",
    "        data_collator=collator_fn,\n",
    "        train_dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3120' max='3120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3120/3120 40:32, Epoch 78/80]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.037200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.917200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.949400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.846900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.912300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.772600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.824700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.748200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.796700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.656600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.703800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.668700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.708400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.589200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.657200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.566800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>1.619500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>1.548200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>1.593400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.509100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>1.581500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>1.472800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>1.553200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>1.442500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.501700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>1.448000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>1.503500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>1.402500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>1.488300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.385100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>1.457600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>1.366700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>1.417300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>1.361600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.445700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>1.314700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>1.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>1.316600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>1.391000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.296600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>1.358900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>1.299300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>1.354000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>1.278700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.348600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>1.255700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>1.308800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>1.270700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>1.318700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.228400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>1.299100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>1.235700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>1.300500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>1.212200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.283000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>1.209700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>1.261800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>1.220600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>1.256900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.196500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>1.258500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>1.186500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>1.241400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>1.176500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.234400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>1.162700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>1.195300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>1.187800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>1.195200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.173900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>1.189600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>1.165900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>1.179900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>1.165300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.171000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>1.157900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1540</td>\n",
       "      <td>1.171900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>1.148000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1580</td>\n",
       "      <td>1.157900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.147700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1620</td>\n",
       "      <td>1.175800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1640</td>\n",
       "      <td>1.127500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1660</td>\n",
       "      <td>1.154700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>1.130300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.171700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1720</td>\n",
       "      <td>1.100500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1740</td>\n",
       "      <td>1.160800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>1.110200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1780</td>\n",
       "      <td>1.158200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.112700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1820</td>\n",
       "      <td>1.171000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>1.087300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1860</td>\n",
       "      <td>1.155200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1880</td>\n",
       "      <td>1.092500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.133200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>1.112200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1940</td>\n",
       "      <td>1.153200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1960</td>\n",
       "      <td>1.083700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1980</td>\n",
       "      <td>1.130800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.096200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020</td>\n",
       "      <td>1.133400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2040</td>\n",
       "      <td>1.088600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2060</td>\n",
       "      <td>1.121200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>1.101600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.129500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2120</td>\n",
       "      <td>1.076700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2140</td>\n",
       "      <td>1.128700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>1.080100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2180</td>\n",
       "      <td>1.103400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.098300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2220</td>\n",
       "      <td>1.147200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>1.062300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2260</td>\n",
       "      <td>1.127300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2280</td>\n",
       "      <td>1.070500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>1.121800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2320</td>\n",
       "      <td>1.071600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2340</td>\n",
       "      <td>1.128100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2360</td>\n",
       "      <td>1.062800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2380</td>\n",
       "      <td>1.109300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.077200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2420</td>\n",
       "      <td>1.109400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2440</td>\n",
       "      <td>1.075000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2460</td>\n",
       "      <td>1.132800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2480</td>\n",
       "      <td>1.048600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.126800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2520</td>\n",
       "      <td>1.052500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2540</td>\n",
       "      <td>1.115900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2560</td>\n",
       "      <td>1.069400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2580</td>\n",
       "      <td>1.114300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.054500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2620</td>\n",
       "      <td>1.106000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2640</td>\n",
       "      <td>1.072000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2660</td>\n",
       "      <td>1.102700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2680</td>\n",
       "      <td>1.067000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>1.109100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2720</td>\n",
       "      <td>1.065800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2740</td>\n",
       "      <td>1.121500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2760</td>\n",
       "      <td>1.046500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2780</td>\n",
       "      <td>1.110500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>1.054400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2820</td>\n",
       "      <td>1.111500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2840</td>\n",
       "      <td>1.061900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2860</td>\n",
       "      <td>1.121000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2880</td>\n",
       "      <td>1.056600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>1.093100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2920</td>\n",
       "      <td>1.076000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2940</td>\n",
       "      <td>1.123600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2960</td>\n",
       "      <td>1.049300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2980</td>\n",
       "      <td>1.106000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.058200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3020</td>\n",
       "      <td>1.127300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3040</td>\n",
       "      <td>1.046000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3060</td>\n",
       "      <td>1.098900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3080</td>\n",
       "      <td>1.068900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>1.109200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3120</td>\n",
       "      <td>1.057400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at ./models/v1.pth!\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "model.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'گزارش عملکرد سرورهای سامانه اتوماسیون اداری سازمان تامین اجتماعی',\n",
       " 'context': '\\n\\nشماره: \\n\\nتاریخ: \\n\\nپیوست: \\nدارد\\nبسمه تعالی\\n\\n\\n\\n\\nجناب آقای مهندس بهروز کتابی\\nمدیر محترم فناوری و تحول دیجیتال سازمان تامین اجتماعی\\nبا سلام و احترام\\nبه پیوست گزارش عملکرد سرورهای سامانه اتوماسیون اداری آن سازمان مربوط به مهرماه سال ۱۴۰۳ حضورتان ارسال می\\u200fگردد. \\n\\n\\n\\nبا تشکر\\nمهدی اسد بگی\\nمعاون امور فنی و پشتیبانی\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " 'context_embedding': '[-0.009723538532853127, -0.00010115613986272365, -0.029605424031615257, -0.02930566854774952, -0.013792633078992367, -0.07930489629507065, 0.009425357915461063, -0.014393379911780357, 0.028125934302806854, 0.055396534502506256, 0.03288242593407631, 0.02731730043888092, 0.030033962801098824, 0.04641319811344147, -0.025476908311247826, 0.022169610485434532, 0.02849527634680271, -0.018998190760612488, -0.011828726157546043, 0.003549319226294756, 0.028123190626502037, 0.02426593191921711, 0.06110379099845886, 0.0042107668705284595, -0.005941149313002825, 0.004545609466731548, -0.021812254562973976, 0.001366627519018948, 0.04233166202902794, 0.026173561811447144, -0.0048941937275230885, -0.013507608324289322, 0.04008392617106438, 0.02595362439751625, -0.058629609644412994, -0.020985908806324005, 0.017536763101816177, -0.1073814257979393, -0.06390443444252014, 0.08453235030174255, 0.022187642753124237, -0.03684242069721222, -0.0017761895433068275, -0.02582450956106186, 0.028842678293585777, -0.03701239451766014, -0.043656088411808014, -0.018139716237783432, -0.04957370460033417, -0.04839639365673065, 0.0014484541025012732, -0.029669636860489845, 0.04690834507346153, -0.03371777758002281, 0.007607226725667715, 0.03610881417989731, -0.06181987747550011, 0.02373036928474903, -0.0834374725818634, 0.02565515786409378, -0.03796715661883354, 0.012731615453958511, 0.03052803874015808, -0.030576590448617935, -0.01592218317091465, 0.07213481515645981, 0.033755071461200714, 0.016979506239295006, 0.022149916738271713, -0.034691955894231796, 0.003562489291653037, 0.007902497425675392, -0.016551068052649498, -0.004972874652594328, -0.06561734527349472, 0.06040105223655701, 0.007967480458319187, -0.007370557636022568, -0.002939053578302264, 0.028567377477884293, 0.008623139932751656, -0.00032356075826101005, 0.008803200908005238, -0.02588309533894062, -0.025737017393112183, 0.0017051654867827892, 0.008630937896668911, 0.10852805525064468, -0.0013892676215618849, 0.003195058787241578, -0.03283314034342766, 0.012898994609713554, 0.0005330934072844684, -0.029814155772328377, -0.052474480122327805, -0.031221376731991768, 0.0020939784590154886, 0.0019287645118311048, 0.03733283653855324, 0.03914700821042061, -0.003778704209253192, -0.05575742945075035, -0.0031834908295422792, -0.006828426383435726, 0.03203808516263962, -0.03187696635723114, 0.05028751119971275, -0.013104785233736038, -0.029975665733218193, 0.01233464665710926, -0.024899903684854507, 0.021526625379920006, 0.04418160393834114, 0.0298310574144125, 0.01698976941406727, -0.021742140874266624, 0.03508121892809868, 0.011110447347164154, 0.01573522947728634, -0.0023679311852902174, 0.04337093606591225, 0.022634707391262054, 0.005461933556944132, -0.0307945404201746, -0.009845924563705921, -0.03952399641275406, -0.007578849792480469, 0.023179231211543083, 0.005578051321208477, -0.009080590680241585, -0.01174058299511671, 0.02593074180185795, -0.018397390842437744, -0.056332066655159, -0.011635311879217625, -0.0017747022211551666, -0.02874428778886795, -0.011855143122375011, 0.016978714615106583, -0.009631792083382607, 0.06384176760911942, 0.05504267290234566, -0.05922713503241539, -0.009382551535964012, 0.011655472218990326, -0.030906440690159798, 0.021302498877048492, -0.0032841966021806, -0.0163322314620018, 0.021731698885560036, 0.020970061421394348, 0.004292318131774664, -0.007119425572454929, 0.013425825163722038, 0.012114730663597584, -0.005703427828848362, 0.02525571547448635, -0.028204545378684998, -0.03783402219414711, -0.007438694592565298, -0.02942214161157608, 0.04177888110280037, -0.022251339629292488, -0.002333891810849309, 0.04800569266080856, -0.01871494948863983, -0.007227235473692417, -0.0010119522921741009, 0.0037491433322429657, 0.018452446907758713, 0.011660242453217506, -0.0376354344189167, -0.04309229552745819, -0.008859722875058651, 0.003921038005501032, 0.0029326758813112974, 0.06438243389129639, -0.002229981357231736, 0.03655104339122772, -0.04341511055827141, -0.01664651557803154, -0.018482288345694542, -0.024330709129571915, -0.0296146422624588, -0.011936529539525509, -0.0023606892209500074, -0.015547361224889755, -0.03364880755543709, 0.003349845064803958, 0.011471730656921864, -0.004559790249913931, -0.03391196206212044, 0.010222438722848892, 0.009759477339684963, -0.03382471948862076, 0.020778371021151543, -0.02293205074965954, -0.009531105868518353, -0.009893556125462055, -0.031893499195575714, 0.0013689846964552999, 0.07429372519254684, -0.009761017747223377, 0.006620919797569513, 0.00957332644611597, -0.008914529345929623, 0.00999441184103489, -0.027594653889536858, 0.007691474165767431, -0.05358579382300377, -0.029144711792469025, 0.02231448143720627, -0.002053583739325404, 0.006518634967505932, -0.034361183643341064, 0.04926684871315956, -0.025522617623209953, 0.010804115794599056, 0.032638080418109894, -0.06306598335504532, -0.014636202715337276, 0.029604200273752213, 0.05154339596629143, 0.023648696020245552, -0.01986752822995186, -0.035312261432409286, 0.007541333790868521, 0.036966919898986816, 0.03078320436179638, -0.0553339459002018, -0.012280336581170559, -0.01287928968667984, -0.0100148506462574, 0.012269393540918827, 0.07640185952186584, -0.0132827740162611, -0.007308498024940491, 0.03347809240221977, 0.053724419325590134, 0.009052257984876633, 2.171550295315683e-05, -0.030483726412057877, 0.008077137172222137, -0.04480154067277908, -0.04765895754098892, 0.0302321407943964, 0.04770739749073982, 0.03714240714907646, -0.04087448492646217, 0.034326791763305664, -0.0007983231917023659, -0.025904234498739243, 0.0390893816947937, 0.01173254381865263, 0.03266068920493126, 0.004343556240200996, 0.018553556874394417, -0.015121369622647762, 0.009923383593559265, -0.03849375247955322, -0.0017221799353137612, 0.008002503775060177, -0.026753360405564308, 0.023429682478308678, 0.01065211370587349, 0.010121424682438374, 0.02533368021249771, -0.022927409037947655, 0.017014140263199806, -0.02287699468433857, -0.022074535489082336, -0.016785982996225357, -0.009861357510089874, 0.006838166620582342, 0.028317129239439964, 0.030706914141774178, 0.01601150818169117, 0.003918915055692196, 0.06369160115718842, 0.010762128978967667, 0.020690888166427612, 0.027130726724863052, 0.0022928821854293346, -0.010181495919823647, 0.069412961602211, -0.019943485036492348, -0.026486027985811234, -0.030237548053264618, 0.0037296353839337826, -0.020158687606453896, -0.024499142542481422, 0.005611896514892578, 0.07670557498931885, 0.002447856590151787, 0.02462458238005638, -0.000998373026959598, -0.020288825035095215, -0.13901975750923157, 0.01701163500547409, -0.04824918881058693, -0.01309951487928629, 0.02461157739162445, 0.0017292469274252653, -0.011445564217865467, -0.004231292754411697, -0.017396366223692894, 0.0214916430413723, -0.0017349593108519912, -0.05045285075902939, 0.02167786844074726, 0.0037491340190172195, -0.041411105543375015, -0.01744295470416546, 0.03307585045695305, 0.0215433482080698, 0.03802205249667168, -0.02968761883676052, 0.008097540587186813, -0.0792279839515686, 0.021971790120005608, -0.054014842957258224, -0.0007241990533657372, -0.03450649976730347, 0.06769891828298569, 0.006840313319116831, -0.06174182891845703, 0.006342886481434107, 0.004336806014180183, -0.03401358053088188, 0.0006971589173190296, 0.0036861991975456476, 0.03614199161529541, 0.03529547154903412, 0.07268017530441284, -0.06989829987287521, -0.01596221886575222, -0.026472771540284157, -0.010619599372148514, 0.012931675650179386, -0.008494185283780098, -0.014992471784353256, 0.028313223272562027, -0.009587215259671211, 0.022577805444598198, -0.023928644135594368, -0.0432254783809185, 0.03604699298739433, -0.013395420275628567, -0.03650202602148056, -0.04363897070288658, 0.010889965109527111, -0.03236496075987816, -0.04057975113391876, -0.03262299299240112, 0.02430294081568718, -0.00873416755348444, -0.007868926972150803, -0.022767936810851097, -0.013694885186851025, -0.0021996269933879375, -0.014410724863409996, -0.06431416422128677, -0.0012285829288884997, 0.027538469061255455, -0.018705083057284355, -0.0010647154413163662, -0.02506507933139801, 0.020264243707060814, -0.0129343681037426, -0.0035251511726528406, 0.006870857905596495, -0.005895768757909536, 0.04676591604948044, -0.00634134141728282, 0.004250354133546352, -0.016292624175548553, -0.10360809415578842, -0.061299022287130356, -0.014086841605603695, -0.06099098548293114, 0.014316538348793983, -0.018150849267840385, -0.012293924577534199, 0.035735126584768295, 0.020978687331080437, 0.00908722821623087, 0.21293099224567413, 0.0004320856824051589, -0.004554304759949446, -0.0025776424445211887, 0.03929596766829491, -0.01503494381904602, -0.03000432252883911, -0.016900986433029175, 0.02098853886127472, -0.029171887785196304, 0.0007433799328282475, -0.017007771879434586, 0.0025617608334869146, 0.02693513222038746, 0.0055351899936795235, 0.025538219138979912, -0.018182197585701942, -0.02536102756857872, 0.042367078363895416, 0.013553952798247337, -0.0004364528867881745, 0.025892117992043495, 0.00037441833410412073, -0.023390430957078934, -0.060470789670944214, -0.022316597402095795, -0.018013034015893936, 0.04464135318994522, -0.0223270021378994, 0.05026044696569443, -0.00474627735093236, -0.0032185674645006657, -0.010933677665889263, 0.019498461857438087, -0.036132220178842545, 0.03620685264468193, 0.03451593592762947, -0.006011815741658211, 0.021789677441120148, 0.014245929196476936, 0.019913027063012123, -0.04839719831943512, -0.03992921859025955, 0.009603677317500114, -0.011048384010791779, -0.004670218098908663, 0.0048683034256100655, 0.0011782776564359665, -0.03835988789796829, -0.0049431114457547665, 0.042250484228134155, -0.0061106253415346146, 0.011287093162536621, -0.015008456073701382, -0.008564949966967106, -0.002896962221711874, 0.006712868344038725, -0.03422683849930763, -0.015546003356575966, 0.010646992363035679, 0.05199068412184715, 0.014646952971816063, -0.007256716955453157, 0.003616239409893751, 0.018231822177767754, -0.011487854644656181, 0.029236093163490295, 0.0045736366882920265, -0.027785224840044975, 0.03489648923277855, -0.012704433873295784, 0.0507904477417469, 0.017190489917993546, -0.013360125944018364, 0.006122661754488945, -0.038441628217697144, 0.02936386875808239, 0.06420929729938507, -0.059592925012111664, 0.059976618736982346, -0.023862384259700775, 0.023215221241116524, -0.04726807773113251, 0.03160157427191734, -0.004829172044992447, -0.028408030048012733, -0.05222485214471817, 0.061639949679374695, 0.013700284995138645, 0.0351138599216938, -0.02335606887936592, 0.00875434372574091, 0.03902258351445198, 0.0006716586649417877, 0.017632771283388138, 0.007663252297788858, 0.008606196381151676, 0.007044889032840729, -0.03604988381266594, -0.019993316382169724, -0.02721147984266281, -0.012886591255664825, -0.008188987150788307, -0.03194021061062813, 0.02430940605700016, 0.022149499505758286, -0.004055421333760023, 0.012079853564500809, -0.0038526724092662334, 0.038587551563978195, -0.008623321540653706, 0.03738654404878616, 0.05341509357094765, -0.021039040759205818, -0.013512608595192432, 0.008930550888180733, -0.00571399787440896, 0.061258938163518906, 0.05706300213932991, -0.06093359738588333, -0.030120519921183586, -0.010755851864814758, -0.03895241767168045, -0.016124911606311798, 0.050192710012197495, -0.033684141933918, -0.00997710321098566, -0.09200603514909744, -0.020697152242064476, 0.02596026472747326, -0.016278358176350594, -0.00838644802570343, 0.04611368104815483, 0.00732411490753293, -0.022701069712638855, 0.016659000888466835, -0.04196958988904953, 0.022764649242162704, -0.011100864969193935, 0.03745419532060623, -0.05307018384337425, 0.011272178031504154, -0.03421657904982567, -0.013240335509181023, 0.0039673070423305035, 0.05070657283067703, 0.0258747898042202, 0.02050681598484516, -0.038087163120508194, -0.02110322192311287, 0.02564328908920288, 0.03667747601866722, 0.04550400748848915, -0.007085462566465139, 0.011874200776219368, -0.055077239871025085, -0.01631356030702591, -0.01951632648706436, 0.09012773633003235, -0.025157148018479347, -0.01591213420033455, -0.0053610121831297874, 0.05515936017036438, 0.03911549225449562, -0.018965573981404305, 0.0530037023127079, -0.04160109534859657, -0.0028407557401806116, 0.046987880021333694, -0.029835106804966927, 0.037300337105989456, -0.05971679836511612, -0.02914685755968094, 0.03808322176337242, 0.03391077369451523, -0.022169141098856926, -0.008920998312532902, 0.025119071826338768, -0.047330062836408615, -0.012746015563607216, 0.015035423450171947, -0.003859348362311721, -0.05097201466560364, -0.01902436465024948, -0.021446313709020615, 0.006708135362714529, 0.02642141655087471, 0.005484143737703562, -0.0248803049325943, -0.019935553893446922, -0.028006337583065033, 0.04762924835085869, 0.014603805728256702, -0.0028621447272598743, 0.018533965572714806, -0.0083787702023983, 0.031554047018289566, 0.05837755650281906, 0.014295762404799461, 0.006896751467138529, -0.03621299937367439, 0.006936390418559313, -0.01694270595908165, -0.01005014218389988, -0.005602046847343445, -0.006032214034348726, -0.004612456075847149, 0.008243190124630928, -0.00818217545747757, 0.007648593280464411, 0.0252541471272707, 0.037730954587459564, -0.02422512136399746, -0.004516486078500748, 0.018818462267518044, -0.05132470279932022, 0.009633967652916908, 0.015620246529579163, 0.017404397949576378, -0.015989815816283226, -0.007077707909047604, 0.019690094515681267, 0.03777094557881355, -0.03769468888640404, -0.06430298089981079, 0.014464053325355053, -0.02243940532207489, -0.030054980888962746, 0.006067574489861727, 0.047360554337501526, 0.00998106598854065, -0.08373285084962845, 0.03815577179193497, -0.015179409645497799, 0.005588927771896124, 0.021277418360114098, -0.014222350902855396, 0.025624243542551994, -0.04049085080623627, -0.03579166531562805, 0.006327951792627573, 0.02252218686044216, 0.05157856643199921, -0.021239101886749268, 0.003154878970235586, 0.010076702572405338, -0.06752114742994308, -0.003061177907511592, -0.009391163475811481, -0.01712486892938614, -0.030156821012496948, -0.02101193740963936, 0.0315416045486927, -0.006514959502965212, -0.004588175565004349, 0.028240667656064034, -0.009796008467674255, -0.030136456713080406, 0.024156250059604645, -0.06651657819747925, -0.02375752106308937, 0.005271072033792734, 0.0033036558888852596, -0.00597282825037837, 0.0034212367609143257, -0.06093056872487068, 0.0014162355801090598, 0.0013039951445534825, -0.034391745924949646, 0.003982248716056347, -0.00828911829739809, -0.01763899251818657, -0.03214243799448013, -0.0011914544738829136, 0.027536559849977493, -0.02195824682712555, -0.00619365694001317, -0.013621884398162365, -0.0010658862302079797, 0.02780161239206791, -0.030144458636641502, -0.008542612195014954, -0.0023108532186597586, 0.03791657090187073, -0.02772103250026703, -0.003243510378524661, 0.03752446919679642, -0.04663201794028282, -0.03776189684867859, 0.029214074835181236, 0.008133905939757824, -0.016103897243738174, -0.02513405680656433, 0.02851545438170433, -0.03284335508942604, 0.0026505249552428722, -0.015690263360738754, -0.03631346672773361, -0.026840416714549065, -0.02807537652552128, -0.01764659769833088, 0.03164343535900116, 0.0013374851550906897, 0.015870556235313416, 0.02847941406071186, 0.0018440216081216931, -0.01857319287955761, -0.034831754863262177, 0.02412828430533409, 0.0038085852283984423, -0.02412957139313221, -0.07302939891815186, 0.03383243456482887, 0.01004841085523367, 0.03848691284656525, -0.04213124141097069, 0.04553695395588875, 0.004415419418364763, -0.018565332517027855, -0.00291734142228961, 0.0020781976636499166, 0.037919703871011734, 0.0013557587517425418, -0.026466121897101402, -0.04215375706553459, 0.022599659860134125, -0.018009820953011513, -0.02046969160437584, -0.0148727772757411, 0.03278109431266785, -0.04536505788564682, -0.04118787869811058, -0.0061631156131625175, 0.002630828181281686, 0.031133675947785378, 0.0039000583346933126, 0.04925728216767311, -0.01956332102417946, 0.04804978147149086, 0.02624266780912876, -0.04321883246302605, -0.03536199405789375, 0.032523080706596375, -0.014437022618949413, 3.5507682696334086e-06, 0.017625771462917328, -0.0073249065317213535, 0.009535262361168861, 0.0020284741185605526, -0.003782415296882391, 0.0009899784345179796, 0.003979815635830164, -0.006096117198467255, -0.024115005508065224, -0.015115845017135143, 0.04320710152387619, 0.01914747804403305, 0.03843538463115692, -0.02344146929681301, -0.0015368897002190351, -0.008783631958067417, 0.017404651269316673, 0.014961951412260532, 0.012540088966488838, 0.0025726372841745615, -4.8112742661032826e-05, 0.0053445459343492985, -0.012047790922224522, 0.012450055219233036, 0.00819158460944891, -0.02064180187880993, 0.010433358140289783, 0.013755141757428646, 0.02585815265774727, -0.018427370116114616, 0.02822348102927208, 0.006127247121185064, 0.005318448878824711, -0.008928723633289337, 0.0016989745199680328, 0.010224612429738045, -0.03595057502388954, -0.005448667332530022, -0.019235093146562576, 0.02049199678003788, -0.020077284425497055, -0.02392149530351162, 0.009236839599907398, 0.03801929950714111, 0.03789902105927467, -0.020422376692295074, -0.007809234783053398, -0.028654111549258232, -0.027170376852154732, -0.15143772959709167, 0.004230428487062454, -0.013059291057288647, -0.05113391578197479, -0.04500927776098251, -0.017952604219317436, -0.030805839225649834, 0.015049007721245289, 0.05628864839673042, -0.014059036038815975, -0.011255739256739616, -0.06643211841583252, 0.002965379972010851, 3.945687296891265e-07, 0.04753909260034561, -0.007555372081696987, 0.03808841481804848, -0.010703769512474537, -0.0543818436563015, 0.11377142369747162, -0.019202107563614845, 0.015764441341161728, 0.0372074618935585, -0.0004889656556770205, 0.008684694766998291, -0.005567044019699097, 0.044998425990343094, 0.007915893569588661, -0.0297721978276968, -0.01460709422826767, -0.01652596890926361, -0.023892264813184738, 0.029071491211652756, 0.024268537759780884, 0.0027978771831840277, 0.017707865685224533, 0.009846324101090431, -0.027946511283516884, -0.003953128587454557, -0.038532357662916183, 0.005849634297192097, 0.0070715248584747314, 0.011933348141610622, 0.009951299987733364, 0.0047899046912789345, 0.03590887039899826, 0.02212158776819706, -0.016897233203053474, -0.036052919924259186, 0.01726546324789524, -0.003334964392706752, 0.0034394757822155952, 0.015478579327464104, -0.015795402228832245, -0.0019211333710700274, -0.01931854337453842, -0.031909599900245667, -0.0019079444464296103, 0.016987266018986702, 0.008194847032427788, -0.04547605663537979, 0.06714147329330444, -0.03467513993382454, -0.014084694907069206, 0.00515246856957674, 0.04172239080071449, -0.03628484532237053, 0.041320864111185074, -0.04577149078249931, 0.016781030222773552, 0.0074540660716593266, 0.002125913742929697, 0.04741542786359787, -0.05495879054069519, -0.04320709779858589, -0.013740415684878826, 0.03431973606348038, -0.016415659338235855, -0.020404046401381493, -0.011242345906794071, -0.021097199991345406, 0.02482171170413494, -0.042552728205919266, 0.04589153081178665, 0.00043819984421133995, 0.03548523038625717, -0.0028274047654122114, -0.0218119528144598, -0.04710565134882927, -0.0020271448884159327, -0.015936395153403282, -0.023480992764234543, -0.008222808130085468, -0.0031900068279355764, -0.005146718584001064, 0.031107552349567413, -0.020571166649460793, -0.03310026973485947, 0.02244947850704193, -0.0022505074739456177, 0.04355771839618683, -0.03192980960011482, -0.02620011940598488, 0.0033417660742998123, 0.0004657594545278698, 0.09463465958833694, 0.03981369361281395, -0.032448142766952515, -0.021644968539476395, -0.019878679886460304, 0.036547500640153885, -0.012062766589224339, 0.03688258305191994, -0.018341079354286194, -0.0759739875793457, -0.030031995847821236, 0.020855072885751724, 0.06501122564077377, -0.020423786714673042, 0.0030326335690915585, 0.03299839049577713, -0.009506392292678356, 0.0020941561087965965, -0.05260619893670082, 0.025477884337306023, 0.021982470527291298, 0.04428013786673546, 0.050366491079330444, -0.013646733947098255, 0.0013388885417953134, 0.0005165764014236629, -0.03240127116441727, 0.033036548644304276, 0.05240340158343315, 0.040929216891527176, 0.03715723007917404, -0.003725694492459297, -0.004569582641124725, -0.05280159413814545, -0.0607752688229084, -0.014162125997245312, 0.0031881213653832674, 0.001594629604369402, -0.014205429702997208, 0.01699543185532093, 0.017057256773114204, 0.006986899767071009, -0.008776070550084114, 0.043033234775066376, -0.019639967009425163, 0.010336006060242653, 0.013320086523890495, -0.015026113949716091, 0.04151543974876404, 0.0025149520952254534, 0.030287329107522964, 0.010762062855064869, -0.003534469287842512, -0.02934482879936695, 0.0025119129568338394, 0.0007798653095960617, -0.009136772714555264, -0.025892915204167366, -0.028251292183995247, -0.009674303233623505, -0.009831632487475872, -0.03562261536717415, 0.0016638962551951408, -0.022850938141345978, 0.012257504276931286, -0.026412460952997208, -0.005148627795279026, -0.027316614985466003, 0.00801843497902155, 0.023084962740540504, 0.004106954671442509, 0.024111753329634666, 0.01588752120733261, 0.06316876411437988, 0.04395723715424538, 0.020375758409500122, 0.021500783041119576, 0.05864289775490761, 0.056845057755708694, -0.02543557807803154, 0.007639478426426649, 0.04394448176026344, -0.02135859988629818, -0.04633672907948494, -0.004055794328451157, 0.006192811299115419, -0.010367069393396378, -0.015224765986204147, 0.021552637219429016, 0.034728702157735825, 0.053554028272628784, 0.029796550050377846, -0.018620647490024567, 0.008473971858620644, 0.029675431549549103, -0.06698029488325119, 0.0014093228382989764, 0.017377082258462906, 0.012956689111888409, 0.03303208574652672, -0.006261310074478388, 0.04399316757917404, -0.050415392965078354, -0.033538319170475006, -0.043283626437187195, -0.03175719827413559, 0.004591087345033884, 0.019202223047614098, 0.0007038264302536845, -0.01181466318666935, -0.00410136254504323, 0.029684944078326225, -0.00916802417486906, -0.03349928557872772, 0.04601049795746803, -0.00016324238094966859, 0.029626389965415, 0.008760429918766022, -0.04033208638429642, -0.03153374791145325, -0.006116149015724659, 0.006311152130365372, 0.024874046444892883, -0.001717785489745438, 0.014970013871788979, -0.017182694748044014, -0.005522813182324171, -0.02056129463016987, 0.05407102033495903, -0.00044668183545581996, 0.0007230642368085682, -0.04488999769091606, -0.010178368538618088, -0.025188354775309563, -0.04298865422606468, -0.01086109783500433, 0.015796342864632607, -0.034108713269233704, -0.029418591409921646]'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data = dataset[0]\n",
    "sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  107, 34247, 34247,   149,   148, 12919,   107,   149,   220, 12919,\n",
      "           220,   107,   220,   220, 12919,   107,   220,   220,   220]],\n",
      "       device='cuda:0')\n",
      "�ا�ا���ا�� ا �  ا�   \n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    letter_emb = model.letter_projection(torch.tensor(json.loads(sample_data[\"context_embedding\"])).to(device).view(1, 1,-1))\n",
    "    output = model.gpt.generate(\n",
    "        inputs_embeds=letter_emb,\n",
    "        attention_mask=torch.ones((1, 1), dtype=torch.long).to(device),\n",
    "        # do_sample=True,\n",
    "        # top_p=0.9,\n",
    "        # temperature=0.9,\n",
    "        # num_beams=5,\n",
    "        # max_length=128,\n",
    "        # min_length=1,\n",
    "        # repetition_penalty=1.0,\n",
    "        # length_penalty=1.0,\n",
    "        # num_return_sequences=1,\n",
    "    )\n",
    "\n",
    "    output_ids = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "    print(output)\n",
    "    print(''.join(output_ids))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
